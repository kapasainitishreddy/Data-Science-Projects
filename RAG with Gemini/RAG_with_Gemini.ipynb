{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an LLM application using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules to interact with Google's Generative AI and LangChain for document handling and QA.\n",
    "import google.generativeai as genai  # Import Google's Generative AI library to generate text with AI models.\n",
    "from IPython.display import display  # Import display function to show output in Jupyter notebooks.\n",
    "from IPython.display import Markdown  # Import Markdown display for better text formatting in Jupyter.\n",
    "import textwrap  # Import textwrap to neatly format text output.\n",
    "\n",
    "# LangChain tools for processing and splitting text from documents like PDFs.\n",
    "from langchain_community.document_loaders import PyPDFLoader  # Load PDF files for processing.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits text into chunks to handle large text.\n",
    "\n",
    "# Importing modules to integrate Google's Generative AI into LangChain and handle AI-powered Q&A.\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI  # Allows the use of Google AI for embeddings (understanding text) and chat.\n",
    "from langchain.vectorstores import Chroma  # Chroma is a vector store that helps store and search through the AI-processed text data.\n",
    "from langchain.chains import RetrievalQA  # Module to create a question-answering system that retrieves the best answers from documents.\n",
    "\n",
    "# Configuring the Google Generative AI API key to use the generative model (you should replace the key with your own).\n",
    "genai.configure(api_key='ur api key')\n",
    "\n",
    "# Defining which AI model to use. Here, we're selecting Google's 'gemini-pro' model.\n",
    "model = genai.GenerativeModel('gemini-pro')  # 'gemini-pro' is a powerful AI model that can generate high-quality text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **GPT-3 has a larger training data set.** GPT-3 was trained on a massive dataset of text and code, while Gemini was trained on a smaller dataset. This gives GPT-3 an advantage in tasks that require a lot of knowledge, such as answering complex questions or generating creative text.\n",
      "* **GPT-3 is more powerful.** GPT-3 has a larger model size than Gemini, which means that it has more parameters and is able to perform more complex computations. This makes GPT-3 better suited for tasks that require a lot of computational power, such as image generation or language translation.\n",
      "* **GPT-3 is more versatile.** GPT-3 can be used for a wider variety of tasks than Gemini. In addition to the tasks mentioned above, GPT-3 can also be used for tasks such as chatbots, code generation, and even music composition.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "* **GPT-3 has a larger training data set.** GPT-3 was trained on a massive dataset of text and code, while Gemini was trained on a smaller dataset. This gives GPT-3 an advantage in tasks that require a lot of knowledge, such as answering complex questions or generating creative text.\n",
       "* **GPT-3 is more powerful.** GPT-3 has a larger model size than Gemini, which means that it has more parameters and is able to perform more complex computations. This makes GPT-3 better suited for tasks that require a lot of computational power, such as image generation or language translation.\n",
       "* **GPT-3 is more versatile.** GPT-3 can be used for a wider variety of tasks than Gemini. In addition to the tasks mentioned above, GPT-3 can also be used for tasks such as chatbots, code generation, and even music composition."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate_content(\"Explain which is better GPT 3 or Gemini in 3 bullet points\")\n",
    "print(response.text)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Zero-Knowledge Proof (ZKP)** is a mathematical technique that allows one party (the prover) to convince another party (the verifier) of the truth of a statement without revealing any additional information.\n",
       "\n",
       "- ZKPs rely on the concept of **knowledge commitments** and **zero-knowledge proofs.** A knowledge commitment is a cryptographic construct that allows the prover to commit to a statement without revealing it. A zero-knowledge proof is a mathematical proof that the prover knows the secret statement without revealing it.\n",
       "\n",
       "- ZKPs have various applications, including anonymity, privacy, fraud prevention, and digital signatures. They enable users to prove their identity, verify the validity of data, or perform transactions without compromising their privacy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = model.generate_content(\"Explain ZKPs in 3 bullet points\")\n",
    "Markdown(response2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts {\n",
      "  text: \"Hi! Give me a recipe to make a margherita pizza from scratch.\"\n",
      "}\n",
      "role: \"user\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "parts {\n",
      "  text: \"**Ingredients:**\\n\\n**For the Pizza Dough:**\\n\\n* 3 cups (360g) all-purpose flour, plus more for dusting\\n* 1 teaspoon (5g) active dry yeast\\n* 1 teaspoon (5g) sugar\\n* 1 teaspoon (5g) salt\\n* 1 cup (240ml) warm water (105-115°F)\\n\\n**For the Pizza Sauce:**\\n\\n* 1 (28-ounce) can crushed tomatoes\\n* 2 cloves garlic, minced\\n* 1/2 teaspoon dried oregano\\n* 1/4 teaspoon dried basil\\n* Salt and pepper to taste\\n\\n**For the Toppings:**\\n\\n* 1 cup (120g) fresh mozzarella cheese, thinly sliced\\n* 1/2 cup (60g) grated Parmesan cheese\\n* 1/4 cup (20g) fresh basil leaves\\n* Olive oil, for drizzling\\n\\n**Instructions:**\\n\\n**To Make the Pizza Dough:**\\n\\n1. In a large bowl, whisk together the flour, yeast, sugar, and salt.\\n2. Gradually add the warm water while stirring until a dough forms.\\n3. Turn the dough out onto a lightly floured surface and knead for 5-7 minutes until it becomes smooth and elastic.\\n4. Form the dough into a ball, place it in an oiled bowl, cover with plastic wrap, and let rise in a warm place for 1 hour, or until doubled in size.\\n\\n**To Make the Pizza Sauce:**\\n\\n1. In a saucepan, combine the crushed tomatoes, minced garlic, oregano, basil, salt, and pepper.\\n2. Bring to a simmer and cook for 15-20 minutes, or until thickened.\\n\\n**To Assemble the Pizza:**\\n\\n1. Preheat oven to 500°F (260°C).\\n2. Punch down the risen dough and divide it in half.\\n3. Roll out each half of the dough into a 12-inch (30cm) circle.\\n4. Spread the pizza sauce evenly over the dough, leaving a 1-inch border.\\n5. Top with the mozzarella cheese, Parmesan cheese, and fresh basil leaves.\\n6. Drizzle with olive oil.\\n\\n**To Bake the Pizza:**\\n\\n1. Bake for 10-12 minutes, or until the cheese is melted and bubbly and the crust is golden brown.\\n2. Let cool for a few minutes before slicing and serving.\\n\\n**Enjoy your homemade margherita pizza!**\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "**For the Pizza Dough:**\n",
      "\n",
      "* 3 cups (360g) all-purpose flour, plus more for dusting\n",
      "* 1 teaspoon (5g) active dry yeast\n",
      "* 1 teaspoon (5g) sugar\n",
      "* 1 teaspoon (5g) salt\n",
      "* 1 cup (240ml) warm water (105-115°F)\n",
      "\n",
      "**For the Pizza Sauce:**\n",
      "\n",
      "* 1 (28-ounce) can crushed tomatoes\n",
      "* 2 cloves garlic, minced\n",
      "* 1/2 teaspoon dried oregano\n",
      "* 1/4 teaspoon dried basil\n",
      "* Salt and pepper to taste\n",
      "\n",
      "**For the Toppings:**\n",
      "\n",
      "* 1 cup (120g) fresh mozzarella cheese, thinly sliced\n",
      "* 1/2 cup (60g) grated Parmesan cheese\n",
      "* 1/4 cup (20g) fresh basil leaves\n",
      "* Olive oil, for drizzling\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "**To Make the Pizza Dough:**\n",
      "\n",
      "1. In a large bowl, whisk together the flour, yeast, sugar, and salt.\n",
      "2. Gradually add the warm water while stirring until a dough forms.\n",
      "3. Turn the dough out onto a lightly floured surface and knead for 5-7 minutes until it becomes smooth and elastic.\n",
      "4. Form the dough into a ball, place it in an oiled bowl, cover with plastic wrap, and let rise in a warm place for 1 hour, or until doubled in size.\n",
      "\n",
      "**To Make the Pizza Sauce:**\n",
      "\n",
      "1. In a saucepan, combine the crushed tomatoes, minced garlic, oregano, basil, salt, and pepper.\n",
      "2. Bring to a simmer and cook for 15-20 minutes, or until thickened.\n",
      "\n",
      "**To Assemble the Pizza:**\n",
      "\n",
      "1. Preheat oven to 500°F (260°C).\n",
      "2. Punch down the risen dough and divide it in half.\n",
      "3. Roll out each half of the dough into a 12-inch (30cm) circle.\n",
      "4. Spread the pizza sauce evenly over the dough, leaving a 1-inch border.\n",
      "5. Top with the mozzarella cheese, Parmesan cheese, and fresh basil leaves.\n",
      "6. Drizzle with olive oil.\n",
      "\n",
      "**To Bake the Pizza:**\n",
      "\n",
      "1. Bake for 10-12 minutes, or until the cheese is melted and bubbly and the crust is golden brown.\n",
      "2. Let cool for a few minutes before slicing and serving.\n",
      "\n",
      "**Enjoy your homemade margherita pizza!**\n",
      "Token count: total_tokens: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start a new chat session with the model, which allows the model to remember the conversation\n",
    "hist = model.start_chat()\n",
    "\n",
    "# Send a message to the AI within the chat session and receive a response, keeping track of the chat history\n",
    "response = hist.send_message(\"Hi! Give me a recipe to make a margherita pizza from scratch.\")\n",
    "\n",
    "# Display the AI's response in Markdown format for better readability\n",
    "Markdown(response.text)\n",
    "\n",
    "# Loop through the entire chat history and print each message (both user and AI responses)\n",
    "for i in hist.history:\n",
    "    print(i)  # Print the full message object (includes both parts like user input and AI response)\n",
    "    print('\\n\\n')  # Add new lines between each message for better readability\n",
    "\n",
    "# Access and print the text part of the AI's response (first part of the message)\n",
    "print(i.parts[0].text)\n",
    "\n",
    "# Count and print the number of tokens in a new message to check token usage\n",
    "token_count = model.count_tokens(\"Now please help me find the nearest supermarket from where I can buy the ingredients.\")\n",
    "print(f\"Token count: {token_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature value 0.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting and decision trees. It builds an ensemble of decision trees, where each tree is trained on a weighted version of the training data. The weights are adjusted based on the errors made by the previous trees, ensuring that subsequent trees focus on correcting the mistakes of their predecessors.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Fraud Detection:** XGBoost can identify fraudulent transactions by analyzing patterns in financial data.\n",
       "* **Customer Churn Prediction:** It can predict the likelihood of customers leaving a service by considering factors such as usage history and demographics.\n",
       "* **Recommendation Systems:** XGBoost can recommend products or services to users based on their past preferences and interactions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the training data and a random subset of features. The final prediction is made by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Image Classification:** Random Forest can classify images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** It can be used for tasks such as text classification, sentiment analysis, and spam detection.\n",
       "* **Medical Diagnosis:** Random Forest can assist in diagnosing diseases by analyzing patient data, such as symptoms, medical history, and test results.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both XGBoost and Random Forest are highly accurate algorithms. However, XGBoost tends to perform slightly better on complex datasets.\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially for large datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees that make up the ensemble.\n",
       "* **Hyperparameter Tuning:** XGBoost requires more hyperparameter tuning than Random Forest, which can be time-consuming.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are both powerful machine learning algorithms with a wide range of applications. XGBoost is particularly well-suited for tasks that require high accuracy and speed, while Random Forest is more appropriate when interpretability is important. The choice between the two algorithms depends on the specific requirements of the problem at hand."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature value 0.25, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a tree-based ensemble learning algorithm that combines multiple weak decision trees to create a strong prediction model. It uses a gradient boosting approach, where each tree is trained on the residuals of the previous tree, focusing on correcting the errors made by the earlier trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Fraud Detection:** Identifying fraudulent transactions by analyzing customer behavior and transaction patterns.\n",
       "* **Customer Churn Prediction:** Predicting the likelihood of customers leaving a service or product based on their past behavior and demographics.\n",
       "* **Recommendation Systems:** Personalizing recommendations for users based on their preferences and past interactions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that creates a multitude of decision trees. Each tree is trained on a different subset of the data and a random subset of features. The final prediction is made by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Image Classification:** Classifying images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** Identifying sentiment in text, extracting key phrases, or performing machine translation.\n",
       "* **Medical Diagnosis:** Assisting doctors in diagnosing diseases by analyzing patient data and medical records.\n",
       "\n",
       "**Key Differences:**\n",
       "\n",
       "* **Training Time:** XGBoost is generally faster to train than Random Forest.\n",
       "* **Accuracy:** XGBoost often achieves higher accuracy than Random Forest, especially on complex datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees.\n",
       "* **Regularization:** XGBoost has built-in regularization techniques to prevent overfitting, while Random Forest requires additional regularization methods.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "Both XGBoost and Random Forest are powerful machine learning algorithms with a wide range of applications. XGBoost is preferred for tasks requiring high accuracy and speed, while Random Forest is more suitable for tasks where interpretability is important. By understanding the concepts and use cases of these algorithms, practitioners can effectively leverage them to solve real-world problems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature value 0.5, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting with decision trees. It is widely used for tasks such as classification, regression, and ranking.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Gradient Boosting:** A technique that builds multiple weak models (e.g., decision trees) sequentially. Each model corrects the errors of the previous one.\n",
       "* **Regularization:** XGBoost uses regularization techniques to prevent overfitting and improve generalization performance.\n",
       "* **Parallelization:** XGBoost can be parallelized, making it suitable for large datasets.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Fraud Detection:** Identifying fraudulent transactions in financial data.\n",
       "* **Customer Churn Prediction:** Predicting customers who are likely to leave a service.\n",
       "* **Recommendation Systems:** Personalizing recommendations based on user preferences.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. It is known for its robustness and ability to handle high-dimensional data.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Ensemble Learning:** Random Forest builds a collection of decision trees, each trained on a different subset of the data.\n",
       "* **Randomness:** The trees are constructed using random subsets of features and data points.\n",
       "* **Majority Voting:** The final prediction is made by combining the predictions of the individual trees, typically using majority voting.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Image Classification:** Classifying images into different categories.\n",
       "* **Natural Language Processing:** Identifying the sentiment of text or extracting key information.\n",
       "* **Medical Diagnosis:** Assisting in diagnosing diseases based on patient data.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Model Type | Gradient Boosting | Ensemble of Decision Trees |\n",
       "| Regularization | Yes | No |\n",
       "| Parallelization | Yes | Limited |\n",
       "| Overfitting Prevention | High | Moderate |\n",
       "| Data Handling | Handles high-dimensional data | Can handle high-dimensional data |\n",
       "| Computational Cost | Higher | Lower |\n",
       "| Interpretability | Lower | Higher |\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are powerful machine learning algorithms with distinct strengths and weaknesses. XGBoost is generally more accurate and efficient for large datasets, while Random Forest is easier to interpret and can handle high-dimensional data. The choice of algorithm depends on the specific task and data characteristics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature value 0.75, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost and Random Forest**\n",
       "\n",
       "* **XGBoost (Extreme Gradient Boosting):** A tree-based ensemble method that combines multiple decision trees to achieve high predictive performance. It uses gradient boosting, where each tree corrects the errors of the previous ones.\n",
       "\n",
       "* **Random Forest:** Another tree-based ensemble method that builds multiple decision trees on different subsets of the data and feature space. Each tree makes predictions, and the final prediction is typically the average or majority vote of the individual trees.\n",
       "\n",
       "**Real-Life Use Cases**\n",
       "\n",
       "**XGBoost:**\n",
       "\n",
       "* **Customer Churn Prediction:** Predicting the likelihood of customers leaving a service or product. XGBoost's ability to handle large datasets and non-linear relationships makes it well-suited for this task.\n",
       "* **Fraud Detection:** Identifying fraudulent transactions based on historical data. XGBoost's speed and accuracy help in real-time fraud detection systems.\n",
       "* **Natural Language Processing (NLP):** Sentiment analysis, text classification, and other NLP tasks. XGBoost's ability to extract features from text makes it a valuable tool in this domain.\n",
       "\n",
       "**Random Forest:**\n",
       "\n",
       "* **Feature Selection:** Identifying the most important features in a dataset. Random Forest's feature importance metric can help in selecting the most influential features for modeling.\n",
       "* **Medical Diagnosis:** Predicting patient diagnoses based on symptoms and medical history. Random Forest's ability to handle high-dimensional data and missing values makes it useful in healthcare.\n",
       "* **Image Classification:** Classifying images into categories, such as object detection or facial recognition. Random Forest's ensemble nature provides robustness and improved accuracy in image analysis tasks.\n",
       "\n",
       "**Key Differences**\n",
       "\n",
       "* **Regularization:** XGBoost uses L1 and L2 regularization to prevent overfitting, while Random Forest relies on feature bagging and tree pruning.\n",
       "* **Speed:** XGBoost is typically faster than Random Forest due to its parallel implementation and efficient gradient boosting algorithm.\n",
       "* **Accuracy:** Both methods can achieve high accuracy, but XGBoost is often considered slightly more accurate, especially on complex datasets.\n",
       "* **Interpretability:** Random Forest is generally more interpretable than XGBoost due to its simpler tree structure and feature importance metric.\n",
       "\n",
       "**Choosing Between XGBoost and Random Forest**\n",
       "\n",
       "The choice between XGBoost and Random Forest depends on the specific task and dataset characteristics:\n",
       "\n",
       "* For large datasets, complex relationships, and high accuracy requirements, XGBoost is often the preferred choice.\n",
       "* For feature selection, interpretability, or when dataset size is a limitation, Random Forest may be more suitable.\n",
       "* In many cases, both methods can be used as complementary ensemble models, combining their strengths to achieve even better performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For temperature value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost: Extreme Gradient Boosting**\n",
       "\n",
       "XGBoost is a powerful ensemble learning algorithm that combines multiple decision trees to make predictions. It is known for its speed, accuracy, and ability to handle large datasets.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Gradient Boosting:** Constructs multiple decision trees sequentially, with each tree correcting the errors of its predecessors.\n",
       "* **Regularization:** Prevents overfitting by penalizing complexity and feature importance.\n",
       "* **Hyperparameter Tuning:** Supports fine-tuning of various parameters to optimize performance for specific datasets.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **E-commerce Recommendation:** Predicting product recommendations based on user preferences and purchasing history.\n",
       "* **Credit Risk Assessment:** Evaluating the likelihood of a loan applicant defaulting on their payment.\n",
       "* **Fraud Detection:** Identifying suspicious transactions or activities with high levels of accuracy.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble method that creates multiple decision trees and makes predictions based on the majority vote or average output of the individual trees.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Bagging (Bootstrap Aggregating):** Training multiple decision trees on random subsets of the data.\n",
       "* **Random Feature Selection:** Selecting a random subset of features for each decision tree to promote diversity.\n",
       "* **Low Variance:** Robust to noise and outliers in the data due to the combination of multiple trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Natural Language Processing:** Classifying and generating text, such as spam detection and sentiment analysis.\n",
       "* **Image Recognition:** Identifying objects, scenes, and faces in images.\n",
       "* **Medical Diagnosis:** Assisting healthcare professionals in diagnosing diseases and predicting patient outcomes.\n",
       "\n",
       "**Comparison**\n",
       "\n",
       "* Both XGBoost and Random Forest are effective ensemble learning algorithms.\n",
       "* XGBoost generally provides higher accuracy due to its gradient boosting approach and optimized regularization.\n",
       "* Random Forest is often simpler to implement and requires less hyperparameter tuning.\n",
       "* They can be used in a wide range of applications, but XGBoost is better suited for tasks requiring high prediction accuracy and speed, while Random Forest is more robust and suitable for highly noisy or complex datasets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for temp in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "  config = genai.types.GenerationConfig(temperature=temp)\n",
    "  result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "  print(f\"\\n\\nFor temperature value {temp}, the results are: \\n\\n\")\n",
    "  display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, generation_config=generation_config)\n",
    "    return response\n",
    "for m_o_tok in [1, 50, 100, 150, 200]:\n",
    "    config = genai.types.GenerationConfig(max_output_tokens=m_o_tok)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor max output token value {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is an ensemble learning algorithm that combines multiple weak decision trees into a strong predictor. It uses gradient boosting, a technique that iteratively builds trees and corrects errors made by previous trees.\n",
       "\n",
       "**Use Cases:**\n",
       "* Credit scoring: Predicting the likelihood of a borrower defaulting on a loan\n",
       "* Fraud detection: Identifying fraudulent transactions\n",
       "* Customer churn prediction: Identifying customers at risk of leaving\n",
       "* Medical diagnosis: Predicting the presence or absence of a disease\n",
       "\n",
       "**Benefits:**\n",
       "* High accuracy and robustness\n",
       "* Efficient training process\n",
       "* Can handle large datasets\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is another ensemble learning algorithm that uses decision trees. It creates multiple decision trees, each built on a different subset of the data and using a different set of randomly selected features. The trees are then combined to make predictions.\n",
       "\n",
       "**Use Cases:**\n",
       "* Object detection: Identifying objects in images or videos\n",
       "* Text classification: Classifying text documents into different categories\n",
       "* Recommendation systems: Providing personalized recommendations\n",
       "* Insurance risk assessment: Evaluating the risk of insurance policies\n",
       "\n",
       "**Benefits:**\n",
       "* High accuracy and generalization ability\n",
       "* Can handle non-linear relationships\n",
       "* Provides insights into feature importance\n",
       "\n",
       "**Real-Life Examples:**\n",
       "\n",
       "* **XGBoost for Credit Scoring:** Equifax Uses XGBoost to develop a credit scoring model that is more accurate and fair than traditional methods.\n",
       "* **Random Forest for Object Detection:** Microsoft uses Random Forest to train object detection models for its HoloLens augmented reality headset.\n",
       "* **XGBoost for Fraud Detection:** PayPal uses XGBoost to detect fraudulent transactions in real-time, reducing losses by millions of dollars.\n",
       "* **Random Forest for Customer Churn Prediction:** Amazon uses Random Forest to predict customer churn and identify customers who are at risk of leaving.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "XGBoost and Random Forest are both powerful ensemble learning algorithms with different strengths and weaknesses:\n",
       "\n",
       "* XGBoost tends to perform better on structured data with numerical features.\n",
       "* Random Forest is more suitable for unstructured data, such as text or images.\n",
       "* XGBoost is more efficient and faster to train than Random Forest.\n",
       "\n",
       "The choice between XGBoost and Random Forest depends on the specific problem and data characteristics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost**\n",
       "\n",
       "XGBoost (Extreme Gradient Boosting) is an ensemble learning algorithm that uses decision trees as base learners. It combines the strengths of multiple decision trees to make more accurate predictions. Gradient boosting is a technique that improves the model's accuracy by iteratively adding weak learners (in this case, decision trees) to the ensemble.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Gradient Boosting:** XGBoost trains multiple decision trees sequentially, where each tree is built to correct the errors of the previous trees.\n",
       "* **Regularization:** XGBoost includes regularization terms to prevent overfitting and improve generalization.\n",
       "* **Scalability:** XGBoost is highly parallelizable, allowing for efficient training on large datasets.\n",
       "* **Feature Importance:** XGBoost provides insights into the importance of different features in the model.\n",
       "\n",
       "**Use Cases:**\n",
       "\n",
       "* **Predicting customer churn:** XGBoost can be used to identify customers at risk of leaving a company by analyzing factors like their purchase history, loyalty data, and demographics.\n",
       "* **Fraud detection:** XGBoost can detect fraudulent transactions by analyzing transaction patterns, device information, and user behavior.\n",
       "* **Credit risk modeling:** XGBoost can assess the risk of loan applicants by considering their credit history, income, and other financial data.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that constructs a multitude of decision trees. Each tree is trained on a different subset of data and features. The predictions of all the individual trees are then combined to make the final prediction.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Bootstrap Aggregation (Bagging):** Random Forest uses bagging to create diverse decision trees. Each tree is trained on a different random sample of data.\n",
       "* **Feature Subset Selection:** Random Forest randomly selects a subset of features at each split in the decision tree construction process.\n",
       "* **Out-of-Bag Error:** Random Forest estimates the model's generalization error by using an out-of-bag sample (data not used to train each tree).\n",
       "* **Robustness:** Random Forest is less prone to overfitting and can handle noisy or missing data.\n",
       "\n",
       "**Use Cases:**\n",
       "\n",
       "* **Image classification:** Random Forest can be used to classify images into different categories by extracting features from the images.\n",
       "* **Natural language processing:** Random Forest can be used for text classification, sentiment analysis, and language modeling.\n",
       "* **Medical diagnosis:** Random Forest can help diagnose diseases by analyzing symptoms, test results, and medical history."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (eXtreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is a powerful machine learning algorithm that combines boosting with decision trees. It leverages gradient boosting, where a sequence of decision trees is built iteratively, with each tree aimed at correcting the errors of its predecessors.\n",
       "\n",
       "**Use Case:** Credit Risk Assessment\n",
       "\n",
       "XGBoost can effectively classify loan applicants into low-risk and high-risk categories. By analyzing historical loan data, it considers factors such as income, credit score, and payment history. The model can accurately predict the probability of loan default, helping banks make informed decisions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that builds multiple decision trees and combines their predictions. It randomly selects a subset of features and data points for each tree, improving the model's robustness and reducing overfitting.\n",
       "\n",
       "**Use Case:** Customer Segmentation\n",
       "\n",
       "Random Forest can be used to segment customers based on their purchase history, demographics, and other attributes. By identifying different customer groups, businesses can tailor marketing campaigns and products to meet their specific needs.\n",
       "\n",
       "**Key Differences:**\n",
       "\n",
       "* **Architecture:** XGBoost uses gradient boosting, while Random Forest uses random sampling.\n",
       "* **Tree Building:** XGBoost optimizes tree growth based on gradients, while Random Forest selects features and samples randomly.\n",
       "* **Performance:** XGBoost often has better accuracy due to its ability to handle complex interactions, but Random Forest is generally faster to train.\n",
       "* **Interpretability:** Random Forest is more interpretable as it can provide feature importance scores, while XGBoost's decision-making process can be more complex.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **XGBoost for Fraud Detection:** XGBoost's gradient boosting capabilities make it effective in identifying fraudulent transactions by analyzing transaction patterns and user behavior.\n",
       "* **Random Forest for Medical Diagnosis:** Random Forest can diagnose diseases from medical data by combining the predictions of multiple decision trees, improving accuracy and providing insights into disease risk factors.\n",
       "* **XGBoost for Sales Forecasting:** XGBoost can forecast sales by considering historical sales data, economic indicators, and competitor activity, providing accurate projections for business planning.\n",
       "* **Random Forest for Social Media Sentiment Analysis:** Random Forest can analyze social media data to detect positive or negative sentiment towards brands, helping businesses monitor public perception and adjust their marketing strategies accordingly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a machine learning algorithm that uses a gradient boosting technique to combine multiple weak learners (e.g., decision trees) into a single, powerful ensemble model. It is known for its scalability, accuracy, and efficient handling of large datasets.\n",
       "\n",
       "**Use Case:**\n",
       "* **Financial Risk Prediction:** XGBoost can help predict default risk for loan applications by leveraging historical data on payments, credit history, and other factors.\n",
       "* **Online Advertising:** XGBoost is used to optimize ad campaigns by predicting the likelihood of a user clicking on an advertisement based on their browsing behavior and demographics.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is a machine learning algorithm that creates an ensemble of decision trees. Each tree is trained on a different subset of the data and makes its own prediction. The final prediction is typically the average or majority vote of the individual trees. Random Forest is known for its stability, robustness, and interpretability.\n",
       "\n",
       "**Use Case:**\n",
       "* **Fraud Detection:** Random Forest can be used to identify fraudulent transactions in financial systems by analyzing patterns in historical data.\n",
       "* **Customer Segmentation:** Random Forest can help identify different customer segments based on their purchase history, demographics, and other attributes.\n",
       "\n",
       "**Key Differences:**\n",
       "\n",
       "| **Feature** | **XGBoost** | **Random Forest** |\n",
       "|---|---|---|\n",
       "| Model Type | Gradient Boosting | Ensemble of Decision Trees |\n",
       "| Scalability | Very scalable | Somewhat scalable |\n",
       "| Accuracy | Typically higher | Typically lower |\n",
       "| interpretability | Lower | Higher |\n",
       "| Training Time | Can be longer | Typically faster |\n",
       "\n",
       "**Application Considerations:**\n",
       "\n",
       "* **Data Size:** XGBoost is better suited for large datasets where scalability is a concern.\n",
       "* **Accuracy:** XGBoost often produces higher accuracy than Random Forest, especially on complex datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, making it easier to understand the model's predictions.\n",
       "* **Training Time:** Random Forest typically has faster training times than XGBoost."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top k value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## XGBoost\n",
       "\n",
       "XGBoost (eXtreme Gradient Boosting) is a powerful ensemble machine learning algorithm based on gradient tree boosting. It has gained significant popularity due to its exceptional performance in a wide range of predictive modeling tasks.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Gradient Tree Boosting:** XGBoost builds a sequence of weak learners (decision trees) that are combined to form a powerful ensemble model. Each tree is trained on a modified dataset that emphasizes the errors made by previous trees.\n",
       "* **Regularization:** XGBoost incorporates multiple regularization techniques, such as L1 and L2 norms, to prevent overfitting and improve generalization performance.\n",
       "* **Objective Function Optimization:** XGBoost uses a second-order optimization technique to find the best combination of trees that minimize a specified objective function (e.g., mean squared error, logistic loss).\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Customer Churn Prediction:** XGBoost has been successfully used to predict the likelihood of customers leaving a service. By identifying high-risk customers, businesses can implement targeted retention campaigns.\n",
       "* **Fraud Detection:** XGBoost is effective in detecting fraudulent transactions based on historical data. Financial institutions use it to reduce losses due to fraud.\n",
       "* **Medical Diagnosis:** XGBoost aids in diagnosing diseases by combining data from various sources, such as medical records and images. It improves accuracy and can assist healthcare professionals in making informed decisions.\n",
       "\n",
       "## Random Forest\n",
       "\n",
       "Random Forest is another popular ensemble learning algorithm that builds multiple decision trees. Unlike XGBoost, it follows a more random approach to tree construction.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Decision Trees:** Random Forest combines a set of decision trees, where each tree makes predictions independently.\n",
       "* **Random Subsampling:** When training each tree, Random Forest randomly selects a subset of samples and features. This helps diversify the ensemble and reduce overfitting.\n",
       "* **Majority Voting:** To make a prediction, Random Forest combines the predictions of all individual trees. The most commonly predicted class or value is the final output.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "\n",
       "* **Stock Market Prediction:** Random Forest has been used to predict stock prices based on historical data and macroeconomic indicators. It helps investors make informed trading decisions.\n",
       "* **Image Classification:** Random Forest is effective in classifying images into different categories. It has applications in fields such as object detection and computer vision.\n",
       "* **Sentiment Analysis:** Random Forest can be used to classify text sentiment (positive, negative, or neutral) based on the words and phrases used. It is valuable for analyzing customer feedback and social media data.\n",
       "\n",
       "## Comparison\n",
       "\n",
       "XGBoost and Random Forest share similarities as ensemble methods, but they differ in their underlying mechanisms:\n",
       "\n",
       "* **Regularization:** XGBoost incorporates regularization techniques while Random Forest does not. This gives XGBoost an advantage in preventing overfitting.\n",
       "* **Tree Optimization:** XGBoost uses a second-order optimization technique to find the optimal combination of trees, while Random Forest combines trees by simple majority voting.\n",
       "* **Computational Complexity:** XGBoost is generally more computationally expensive than Random Forest due to its iterative gradient boosting process.\n",
       "\n",
       "Ultimately, the choice between XGBoost and Random Forest depends on the specific application and data characteristics. XGBoost is preferred when regularization and precise optimization are crucial, while Random Forest is suitable for simple and fast predictions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for k in [1, 4, 16, 32, 40]:\n",
    "    config = genai.types.GenerationConfig(top_k=k)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor top k value {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting and decision trees. It builds an ensemble of decision trees, where each tree is trained on a weighted version of the training data. The weights are adjusted based on the errors made by the previous trees, ensuring that subsequent trees focus on correcting the mistakes of their predecessors.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Fraud Detection:** XGBoost can identify fraudulent transactions by analyzing patterns in financial data.\n",
       "* **Customer Churn Prediction:** It can predict the likelihood of customers leaving a service by considering factors such as usage history and demographics.\n",
       "* **Medical Diagnosis:** XGBoost can assist in diagnosing diseases by analyzing medical records and identifying patterns that are indicative of specific conditions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the training data and a random subset of features. The final prediction is made by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Image Classification:** Random Forest can classify images into different categories by analyzing pixel values and extracting features.\n",
       "* **Natural Language Processing:** It can be used for tasks such as text classification, sentiment analysis, and spam detection.\n",
       "* **Financial Forecasting:** Random Forest can predict financial trends by analyzing historical data and identifying patterns.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both XGBoost and Random Forest are highly accurate algorithms, but XGBoost tends to perform slightly better on complex datasets.\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially for large datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees that make up the ensemble.\n",
       "* **Hyperparameter Tuning:** XGBoost requires more hyperparameter tuning than Random Forest, which can be time-consuming.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are both powerful machine learning algorithms with a wide range of applications. XGBoost is particularly well-suited for complex datasets and tasks where accuracy is paramount. Random Forest is a good choice for tasks where interpretability is important or when the dataset is relatively small."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a powerful machine learning algorithm that combines the principles of gradient boosting and decision trees. It builds an ensemble of decision trees, where each tree is trained on a weighted version of the training data. The weights are adjusted based on the errors made by the previous trees, ensuring that subsequent trees focus on correcting the mistakes of their predecessors.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Fraud Detection:** XGBoost can identify fraudulent transactions by analyzing patterns in financial data.\n",
       "* **Customer Churn Prediction:** It can predict the likelihood of customers leaving a service by considering factors such as usage history and demographics.\n",
       "* **Medical Diagnosis:** XGBoost can assist in diagnosing diseases by analyzing medical records and identifying patterns that are indicative of specific conditions.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees. Each tree is trained on a different subset of the training data and a random subset of features. The final prediction is made by combining the predictions of all the individual trees.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Image Classification:** Random Forest can classify images into different categories by analyzing pixel values and extracting features.\n",
       "* **Natural Language Processing:** It can be used for tasks such as text classification, sentiment analysis, and spam detection.\n",
       "* **Financial Forecasting:** Random Forest can predict financial trends by analyzing historical data and identifying patterns.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "* **Accuracy:** Both XGBoost and Random Forest are highly accurate algorithms, but XGBoost tends to perform slightly better on complex datasets.\n",
       "* **Speed:** XGBoost is generally faster than Random Forest, especially for large datasets.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees that make up the ensemble.\n",
       "* **Hyperparameter Tuning:** XGBoost requires more hyperparameter tuning than Random Forest, which can be time-consuming.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "XGBoost and Random Forest are both powerful machine learning algorithms with a wide range of applications. XGBoost is particularly well-suited for complex datasets and tasks where accuracy is paramount. Random Forest is a good choice for tasks where interpretability is important or when the dataset is relatively small."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (Extreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "XGBoost is a machine learning algorithm that combines multiple weak learners (decision trees) into a strong learner. It uses a gradient boosting approach, where each tree is trained on the residuals of the previous tree.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Fraud Detection:** Identifying fraudulent transactions by analyzing historical data.\n",
       "* **Customer Churn Prediction:** Predicting which customers are likely to cancel their subscriptions.\n",
       "* **Stock Market Forecasting:** Predicting future stock prices based on historical data and market trends.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "Random Forest is an ensemble learning algorithm that creates multiple decision trees and combines their predictions. Each tree is trained on a different subset of the data and a random subset of features.\n",
       "\n",
       "**Real-Life Use Cases:**\n",
       "* **Image Classification:** Classifying images into different categories, such as animals, objects, or scenes.\n",
       "* **Natural Language Processing:** Identifying sentiment in text data or extracting key information.\n",
       "* **Medical Diagnosis:** Predicting the likelihood of a patient having a specific disease based on their symptoms and medical history.\n",
       "\n",
       "**Comparison**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Model Type | Gradient Boosting | Ensemble of Decision Trees |\n",
       "| Regularization | Yes | Yes |\n",
       "| Feature Importance | Yes | Yes |\n",
       "| Scalability | High | High |\n",
       "| Interpretability | Lower | Higher |\n",
       "| Computational Cost | Higher | Lower |\n",
       "\n",
       "**When to Use Each Algorithm**\n",
       "\n",
       "* **XGBoost:** When high accuracy and performance are crucial, even at the cost of interpretability.\n",
       "* **Random Forest:** When interpretability and robustness are important, and the computational cost is a concern.\n",
       "\n",
       "**Example Use Case**\n",
       "\n",
       "**Fraud Detection with XGBoost:**\n",
       "\n",
       "A financial institution wants to identify fraudulent transactions. They have a dataset of historical transactions, including features such as transaction amount, merchant category, and customer location.\n",
       "\n",
       "* **Data Preparation:** The data is cleaned and preprocessed to remove outliers and missing values.\n",
       "* **Model Training:** An XGBoost model is trained on the data, using a gradient boosting approach.\n",
       "* **Model Evaluation:** The model is evaluated on a holdout dataset to assess its accuracy and performance.\n",
       "* **Deployment:** The trained model is deployed into production to identify fraudulent transactions in real-time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost (eXtreme Gradient Boosting)**\n",
       "\n",
       "XGBoost is a scalable and powerful ensemble learning algorithm that combines multiple weak learners (e.g., decision trees) into a single, strong learner. It is widely used in machine learning competitions and has achieved state-of-the-art results in various tasks.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Gradient Boosting:** XGBoost uses gradient boosting to iteratively build an ensemble of decision trees. Each tree is trained on the residuals (errors) of the previous tree, focusing on areas where the ensemble is weak.\n",
       "* **Regularization:** XGBoost includes L1 and L2 regularization to prevent overfitting. This helps control the complexity of the ensemble and improve generalization performance.\n",
       "* **Tree Pruning:** XGBoost uses tree pruning to remove unnecessary branches and reduce the model's size without compromising accuracy.\n",
       "* **Parallelization:** XGBoost is highly parallelizable, allowing for efficient training on large datasets.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "\n",
       "* Predicting customer churn in a telecommunications company. XGBoost can analyze customer data (e.g., usage patterns, demographics) to identify factors that contribute to churn and develop a model to predict future churn.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "Random Forest is an ensemble learning algorithm that combines multiple decision trees into a single, more robust model. It is known for its accuracy, flexibility, and ease of use.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Bootstrap Sampling:** Random Forest trains each decision tree on a different subset of the training data. This helps reduce overfitting and improve the model's generalization ability.\n",
       "* **Random Feature Selection:** Random Forest randomly selects a subset of features at each node of each decision tree. This helps prevent overreliance on any particular feature and improves the model's robustness.\n",
       "* **Majority Voting:** Random Forest combines the predictions of all the individual decision trees using majority voting. This helps reduce variance and improve the overall accuracy of the model.\n",
       "\n",
       "**Real-Life Use Case:**\n",
       "\n",
       "* Detecting fraud in financial transactions. Random Forest can analyze transaction data (e.g., amount, location, time) to identify patterns and anomalies that indicate fraudulent activities.\n",
       "\n",
       "**Comparison:**\n",
       "\n",
       "Both XGBoost and Random Forest are powerful ensemble learning algorithms, but they have different strengths and weaknesses:\n",
       "\n",
       "* **Accuracy:** XGBoost is generally more accurate than Random Forest, especially on complex and high-dimensional datasets.\n",
       "* **Speed:** Random Forest is typically faster to train than XGBoost.\n",
       "* **Interpretability:** Random Forest is more interpretable than XGBoost, as it is easier to understand the individual decision trees in the ensemble.\n",
       "* **Hyperparameter Tuning:** XGBoost has more hyperparameters to tune than Random Forest, which can make it more challenging to optimize."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For top p value 1.0, the results are: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**XGBoost and Random Forest: Overview**\n",
       "\n",
       "**XGBoost (Extreme Gradient Boosting)** and Random Forest are two powerful machine learning algorithms used for regression and classification tasks. Both algorithms involve ensemble methods, combining multiple decision trees to make predictions.\n",
       "\n",
       "**XGBoost** is a gradient boosting algorithm that iteratively adds decision trees to an ensemble, with each tree learning from the errors of the previous ones. It uses regularized objectives to prevent overfitting and supports both continuous and categorical features.\n",
       "\n",
       "**Random Forest** constructs multiple decision trees independently and makes predictions by combining the outputs of these trees. It uses random subsets of features and data points to create each tree, resulting in a diverse ensemble.\n",
       "\n",
       "**Real-Life Use Cases**\n",
       "\n",
       "**XGBoost**\n",
       "\n",
       "* **Fraud Detection:** Identifying fraudulent transactions in financial data (features: transaction amount, account type, time of day).\n",
       "* **Cancer Detection:** Classifying medical images as cancerous or benign (features: tumor size, shape, texture).\n",
       "* **Natural Language Processing:** Sentiment analysis and text classification (features: word frequency, sentence structure).\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "* **Image Recognition:** Classifying images into different categories (features: pixel values, shape descriptors).\n",
       "* **Customer Churn Prediction:** Identifying customers at risk of leaving a service (features: customer demographics, account history).\n",
       "* **Stock Market Prediction:** Forecasting future stock prices (features: historical prices, economic indicators).\n",
       "\n",
       "**Advantages and Disadvantages**\n",
       "\n",
       "**XGBoost**\n",
       "\n",
       "* **Advantages:**\n",
       "    * High accuracy and speed\n",
       "    * Handles large datasets and complex interactions\n",
       "    * Built-in regularization to prevent overfitting\n",
       "* **Disadvantages:**\n",
       "    * Can be computationally expensive\n",
       "    * Requires tuning of hyperparameters\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "* **Advantages:**\n",
       "    * Fast and easy to train\n",
       "    * Robust to overfitting\n",
       "    * Handles categorical and missing values well\n",
       "* **Disadvantages:**\n",
       "    * May not perform as well as other algorithms for complex tasks\n",
       "    * Can be unstable due to random feature selection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for p in [0, 0.2, 0.4, 0.8, 1]:\n",
    "    config = genai.types.GenerationConfig(top_p=p)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor top p value {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**XGBoost (eXtreme Gradient Boosting)**\n",
       "\n",
       "**Concept:**\n",
       "* Ensemble learning algorithm that combines multiple decision trees into a single predictive model.\n",
       "* Utilizes a gradient boosting approach to minimize loss function and improve model accuracy.\n",
       "* Supports various tree-building parameters, regularization techniques, and parallelization for faster training.\n",
       "\n",
       "**Use Case:**\n",
       "* Fraud detection: XGBoost can analyze transaction data to identify suspicious patterns indicative of fraud.\n",
       "* Disease prediction: By leveraging patient health records, XGBoost can predict the likelihood of developing certain diseases based on risk factors.\n",
       "\n",
       "**Random Forest**\n",
       "\n",
       "**Concept:**\n",
       "* Ensemble learning algorithm that generates multiple decision trees from different subsets of data.\n",
       "* Each tree independently makes a prediction, and the final prediction is determined by combining the predictions of all trees.\n",
       "* Built using bagging and random feature selection to reduce overfitting and improve generalization.\n",
       "\n",
       "**Use Case:**\n",
       "* Customer churn prediction: Random forest can identify customers at risk of leaving a service based on their usage patterns and demographics.\n",
       "* Image classification: By analyzing pixel features, random forest can classify images into different categories, such as animals, vehicles, or objects.\n",
       "\n",
       "**Key Differences**\n",
       "\n",
       "| Feature | XGBoost | Random Forest |\n",
       "|---|---|---|\n",
       "| Tree building | Gradient boosting | Bagging |\n",
       "| Regularization | Yes | No |\n",
       "| Parallelization | Yes | No |\n",
       "| Model interpretability | Lower | Higher |\n",
       "| Computational cost | Higher | Lower |\n",
       "| Sensitivity to hyperparameters | Higher | Lower |\n",
       "\n",
       "**Advantages of XGBoost**\n",
       "\n",
       "* Higher accuracy due to gradient boosting\n",
       "* Robust to overfitting with regularization techniques\n",
       "* Scalable to large datasets with parallelization\n",
       "\n",
       "**Advantages of Random Forest**\n",
       "\n",
       "* Relatively easy to interpret\n",
       "* Less sensitive to hyperparameter tuning\n",
       "* Can handle both categorical and continuous features"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = genai.types.GenerationConfig(candidate_count=1)\n",
    "result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "Markdown(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG\n",
    "CHUNK_SIZE = 700\n",
    "CHUNK_OVERLAP = 100\n",
    "pdf_path = \"https://www.analytixlabs.co.in/assets/pdfs/Data_Engineering%20&_Other_Job_Roles-AnalytixLabs.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_path)\n",
    "split_pdf_document = pdf_loader.load_and_split()\n",
    "# Splitting text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in split_pdf_document)\n",
    "texts = text_splitter.split_text(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini Pro Chat model from the LangChain Google Generative AI library\n",
    "# 'model' specifies which AI model to use ('gemini-pro'), and 'google_api_key' is your Google API key for authentication\n",
    "# 'temperature=0.8' controls the randomness in response generation. Higher values (closer to 1) make the output more creative or random, while lower values (closer to 0) make it more focused and deterministic.\n",
    "gemini_model = ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=\"ur api key\", temperature=0.8)\n",
    "\n",
    "# Initialize Google Generative AI Embeddings for transforming text data into vectors (numeric representations)\n",
    "# 'model' specifies the embedding model to use, and 'google_api_key' is for authenticating access to Google’s embedding API.\n",
    "# This is used to convert text into vectors so it can be efficiently searched and compared.\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=\"your api key\")\n",
    "\n",
    "# Create a vector index using Chroma, which stores the vector representations of the provided texts\n",
    "# 'from_texts' takes a list of texts and converts them into vector embeddings using the 'embeddings' model.\n",
    "# Chroma is a vector store from the LangChain library that stores and indexes these embeddings for fast similarity search.\n",
    "vector_index = Chroma.from_texts(texts, embeddings)\n",
    "\n",
    "# Create a retriever from the vector index, which retrieves the top 5 most similar pieces of text based on the vector embeddings\n",
    "# 'search_kwargs={\"k\" : 3}' limits the retriever to return the top 5 most similar text chunks when queried.\n",
    "retriever = vector_index.as_retriever(search_kwargs={\"k\" : 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(gemini_model, retriever=retriever, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1. **Data lakes:** Data lakes are central repositories for storing vast amounts of structured and unstructured data. Data engineers use data lakes to consolidate data from various sources, making it accessible for analysis and processing.\n",
      "2. **Data warehouses:** Data warehouses are optimized for storing and querying large volumes of structured data. Data engineers design and implement data warehouses to support business intelligence and reporting.\n",
      "3. **Data pipelines:** Data pipelines are automated processes that move data from one system to another. Data engineers create data pipelines to extract, transform, and load data into data warehouses or data lakes.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which 3 tools do Data Engineers primarily work with?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(\"Answer:\", result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
